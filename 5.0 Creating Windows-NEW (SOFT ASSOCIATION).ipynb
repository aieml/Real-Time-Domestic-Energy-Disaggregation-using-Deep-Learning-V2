{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "import keras\n",
    "\n",
    "logs_base_dir=\"logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs2 (started 16:50:45 ago; pid 4940)\n",
      "  - port 6006: logdir logs (started 17:32:55 ago; pid 9348)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.30313293e+09 2.22200000e+02 6.00000000e+00]\n",
      "(1575991, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path='data/APPENDED'\n",
    "data_path='data'\n",
    "ref_dataset= np.load(os.path.join(path,'refrigerator.dat.npy'))\n",
    "print(ref_dataset[0])\n",
    "ref_bins=4\n",
    "ref_window_size=2401\n",
    "input_shape=(2401, 1)\n",
    "\n",
    "ref_dataset=ref_dataset[:,1:]\n",
    "print(ref_dataset.shape)\n",
    "\n",
    "bin_max_count=[0 for i in range(ref_bins)]\n",
    "ref_clusters=[6.64953227,193.52627165,456.50688195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def load_cnn():\n",
    "    \n",
    "    model=Sequential()\n",
    "    #kernel_regularizer=regularizers.l2(0.01)\n",
    "    model.add(Conv1D(filters=8,kernel_size=5,input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=8,kernel_size=3))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=8,kernel_size=3))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Conv1D(filters=16,kernel_size=3))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=16,kernel_size=3))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=16,kernel_size=3))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "#    model.add(Conv1D(filters=32,kernel_size=3))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Conv1D(filters=32,kernel_size=3))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Conv1D(filters=32,kernel_size=3))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Activation('relu'))\n",
    "#    model.add(MaxPooling1D(pool_size=4))\n",
    "#    model.add(Conv1D(filters=64,kernel_size=3,kernel_regularizer=regularizers.l2(0.01)))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Conv1D(filters=64,kernel_size=3,kernel_regularizer=regularizers.l2(0.01)))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Conv1D(filters=64,kernel_size=3,kernel_regularizer=regularizers.l2(0.01)))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(Activation('relu'))\n",
    "#    model.add(MaxPooling1D(pool_size=4))    \n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "#    model.add(Dense(256,activation='relu'))\n",
    "#    model.add(Dense(128,activation='relu'))\n",
    "#    model.add(Dense(64,activation='relu'))\n",
    "#    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "    from keras.optimizers import Adam\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    decay_rate = 1e-6\n",
    "    momentum = 0.8\n",
    "    opt= Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay_rate)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Epoch 1/10\n",
      "250000/250000 [==============================] - 570s 2ms/step - loss: 0.0952 - accuracy: 0.9643\n",
      "Epoch 2/10\n",
      "250000/250000 [==============================] - 565s 2ms/step - loss: 0.0508 - accuracy: 0.9837\n",
      "Epoch 3/10\n",
      "250000/250000 [==============================] - 567s 2ms/step - loss: 0.0395 - accuracy: 0.9886\n",
      "Epoch 4/10\n",
      "250000/250000 [==============================] - 562s 2ms/step - loss: 0.0348 - accuracy: 0.9909\n",
      "Epoch 5/10\n",
      " 90080/250000 [=========>....................] - ETA: 6:07 - loss: 0.0325 - accuracy: 0.9919"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import normalize\n",
    "import csv\n",
    "\n",
    "\n",
    "bin_dict=collections.Counter(ref_dataset[:,1])\n",
    "\n",
    "ref_data=[]\n",
    "ref_target=[]\n",
    "\n",
    "model=load_cnn()\n",
    "\n",
    "\n",
    "k=0\n",
    "count=0\n",
    "graph_no=0\n",
    "key_min = min(bin_dict.keys(), key=(lambda k: bin_dict[k]))\n",
    "ref_min_bin=bin_dict[key_min]\n",
    "print(ref_min_bin)\n",
    "\n",
    "epo=0\n",
    "while(epo<20):\n",
    "    graph_no=0\n",
    "    for i in range(ref_dataset.shape[0]-ref_window_size-1-200000):\n",
    "        ref_data.append(ref_dataset[i:i+ref_window_size,0])\n",
    "\n",
    "        appliance=ref_dataset[i+ref_window_size-1,1]\n",
    "        #print(appliance)\n",
    "\n",
    "        total=np.sum(np.array([1/np.power(ref_clusters[j]-appliance,2) for j in range (len(ref_clusters))]))\n",
    "        prob0=(1/np.power(ref_clusters[0]-appliance,2))/total\n",
    "        prob1=(1/np.power(ref_clusters[1]-appliance,2))/total\n",
    "        prob2=(1/np.power(ref_clusters[2]-appliance,2))/total\n",
    "        #prob3=(1/np.power(ref_clusters[3]-appliance[i],2))/total    \n",
    "        #print(appliance,[prob0,prob1,prob2],np.sum([prob0*[ref_clusters[0]],prob1*[ref_clusters[1]],prob2*[ref_clusters[2]]]))\n",
    "        ref_target.append([prob0,prob1,prob2])\n",
    "        k=k+1\n",
    "        if(k==250000):\n",
    "            data=np.array(ref_data)/500.0\n",
    "            data = normalize(data, axis=1)\n",
    "            target=np.array(ref_target)\n",
    "            data=data.reshape(data.shape[0],data.shape[1],1)\n",
    "            #print(target.shape,data[0],target[0])\n",
    "\n",
    "            if(count!=10):\n",
    "                logdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "                tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "                mc = keras.callbacks.ModelCheckpoint('weights/weights{epoch:08d}.h5', save_weights_only=True)\n",
    "                history=model.fit(data,target,epochs=10,callbacks=[mc]) \n",
    "                \n",
    "                for layer in model.layers:\n",
    "                    weights = layer.get_weights()\n",
    "                    print(weights)\n",
    "                ref_data=[]\n",
    "                ref_target=[]\n",
    "                k=0\n",
    "                count=count+1\n",
    "\n",
    "            else:\n",
    "    #            ref_data=[]\n",
    "    #            ref_target=[]        \n",
    "    #            for i in range(10000):\n",
    "    #                ref_data.append(ref_dataset[700000+i:700000+i+ref_window_size,0])\n",
    "\n",
    "    #                appliance=ref_dataset[700000+i+ref_window_size-1,1]\n",
    "                    #print(appliance)\n",
    "\n",
    "    #                total=np.sum(np.array([1/np.power(ref_clusters[j]-appliance,2) for j in range (len(ref_clusters))]))\n",
    "    #                prob0=(1/np.power(ref_clusters[0]-appliance,2))/total\n",
    "    #                prob1=(1/np.power(ref_clusters[1]-appliance,2))/total\n",
    "    #                prob2=(1/np.power(ref_clusters[2]-appliance,2))/total\n",
    "                    #prob3=(1/np.power(ref_clusters[3]-appliance[i],2))/total    \n",
    "                    #print(appliance,[prob0,prob1,prob2],np.sum([prob0*[ref_clusters[0]],prob1*[ref_clusters[1]],prob2*[ref_clusters[2]]]))\n",
    "\n",
    "    #                ref_target.append([prob0,prob1,prob2])\n",
    "\n",
    "    #            data=np.array(ref_data)/500.0\n",
    "    #            data = normalize(data, axis=1)\n",
    "    #            target=np.array(ref_target)\n",
    "    #            data=data.reshape(data.shape[0],data.shape[1],1)\n",
    "\n",
    "                \n",
    "                print(model.evaluate(data,target),epo,graph_no)\n",
    "                results=model.predict(data)\n",
    "                plt.plot(np.argmax(target,axis=1),'b',label='ACTUAL')\n",
    "                plt.plot(np.argmax(results,axis=1),'r--',label='PREDICTED')\n",
    "                plt.legend()\n",
    "                plt.savefig('graphs_soft_association/'+str(epo)+str(graph_no)+'.jpg')\n",
    "                plt.close()\n",
    "\n",
    "                ref_data=[]\n",
    "                ref_target=[]\n",
    "                k=0\n",
    "                count=0\n",
    "                graph_no=graph_no+1\n",
    "                \n",
    "                #print(target.shape)\n",
    "                #print(results.shape)\n",
    "                actual=np.argmax(target,axis=1)\n",
    "                predicted=np.argmax(results,axis=1)\n",
    "                \n",
    "                tp=actual[predicted>0]\n",
    "                tp_count=len(np.where(tp>0)[0])\n",
    "                fp_count=len(np.where(tp==0)[0])\n",
    "                \n",
    "                tn=actual[predicted==0]\n",
    "                tn_count=len(np.where(tn==0)[0])\n",
    "                fn_count=len(np.where(tn>0)[0])\n",
    "                \n",
    "                p=len(np.where(actual>0)[0])\n",
    "                n=len(np.where(actual==0)[0])\n",
    "                try:\n",
    "                    recall=tp_count/(tp_count+fn_count)*100\n",
    "                    precision=tp_count/(tp_count+fp_count)*100\n",
    "                    accuracy=(tp_count+tn_count)/(p+n)*100\n",
    "                    f1=2*(precision*recall)/(precision+recall)\n",
    "                \n",
    "                    print(\"==========================window \"+str(i)+\"-\"+str(i+ref_window_size)+\"==========================\")\n",
    "                    print(\"TP:\",tp_count,\"FP:\",fp_count,\"TN:\",tn_count,\"FN:\",fn_count,\"P:\",p,\"N:\",n)\n",
    "                    print('Recall:',recall,\"Precision:\",precision,\"Accuaracy:\",accuracy,\"F1:\",f1)\n",
    "                except:\n",
    "                    pass\n",
    "    epo=epo+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_target,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_target,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Total params: 43,891\n",
    "Trainable params: 43,171\n",
    "Non-trainable params: 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
