{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.30313293e+09 2.22200000e+02 6.00000000e+00]\n",
      "(1575991, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path='data/APPENDED'\n",
    "data_path='data'\n",
    "ref_dataset= np.load(os.path.join(path,'refrigerator.dat.npy'))\n",
    "print(ref_dataset[0])\n",
    "ref_bins=4\n",
    "ref_window_size=2401\n",
    "input_shape=(2401, 1)\n",
    "\n",
    "ref_dataset=ref_dataset[:,1:]\n",
    "print(ref_dataset.shape)\n",
    "\n",
    "bin_max_count=[0 for i in range(ref_bins)]\n",
    "ref_clusters=[6.64953227,193.52627165,456.50688195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def load_cnn():\n",
    "    \n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=8,kernel_size=5,input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=8,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=8,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Conv1D(filters=16,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=16,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=16,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Conv1D(filters=32,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=32,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=32,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Conv1D(filters=64,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=64,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=64,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))    \n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "#    model.add(Dense(256,activation='relu'))\n",
    "#    model.add(Dense(128,activation='relu'))\n",
    "#    model.add(Dense(64,activation='relu'))\n",
    "#    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "    from keras.optimizers import Adam\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    decay_rate = 1e-6\n",
    "    momentum = 0.8\n",
    "    opt= Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay_rate)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import normalize\n",
    "import csv\n",
    "\n",
    "\n",
    "bin_dict=collections.Counter(ref_dataset[:,1])\n",
    "\n",
    "ref_data=[]\n",
    "ref_target=[]\n",
    "\n",
    "model=load_cnn()\n",
    "k=0\n",
    "count=0\n",
    "graph_no=0\n",
    "key_min = min(bin_dict.keys(), key=(lambda k: bin_dict[k]))\n",
    "ref_min_bin=bin_dict[key_min]\n",
    "print(ref_min_bin)\n",
    "\n",
    "epo=0\n",
    "\n",
    "\n",
    "for i in range(ref_dataset.shape[0]-ref_window_size-1-200000):\n",
    "    ref_data=ref_dataset[i:i+ref_window_size,0]\n",
    "\n",
    "    appliance=ref_dataset[i+ref_window_size-1,1]\n",
    "    #print(appliance)\n",
    "\n",
    "    total=np.sum(np.array([1/np.power(ref_clusters[j]-appliance,2) for j in range (len(ref_clusters))]))\n",
    "    prob0=(1/np.power(ref_clusters[0]-appliance,2))/total\n",
    "    prob1=(1/np.power(ref_clusters[1]-appliance,2))/total\n",
    "    prob2=(1/np.power(ref_clusters[2]-appliance,2))/total\n",
    "    #prob3=(1/np.power(ref_clusters[3]-appliance[i],2))/total    \n",
    "    #print(appliance,[prob0,prob1,prob2],np.sum([prob0*[ref_clusters[0]],prob1*[ref_clusters[1]],prob2*[ref_clusters[2]]]))\n",
    "    ref_target=[prob0,prob1,prob2]\n",
    "\n",
    "    data=np.array(ref_data)/500.0\n",
    "    target=np.array(ref_target)\n",
    "    \n",
    "    with open('data/WINDOWED_NEW/ref_data.dat', mode='a',newline=\"\") as out_file:\n",
    "        writer = csv.writer(out_file, delimiter=',')\n",
    "        writer.writerow(data)\n",
    "    out_file.close()\n",
    "    \n",
    "    with open('data/WINDOWED_NEW/ref_target.dat', mode='a',newline=\"\") as out_file:\n",
    "        writer = csv.writer(out_file, delimiter=',')\n",
    "        writer.writerow(target)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(data,target))\n",
    "results=model.predict(data)\n",
    "plt.plot(np.argmax(target,axis=1),'b',label='ACTUAL')\n",
    "plt.plot(np.argmax(results,axis=1),'r--',label='PREDICTED')\n",
    "plt.legend()\n",
    "plt.savefig('graphs_soft_association/'+str(epo)+str(graph_no)+'.jpg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_target,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_target,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Total params: 43,891\n",
    "Trainable params: 43,171\n",
    "Non-trainable params: 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
